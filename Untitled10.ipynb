{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled10.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1TdO8HQY8q9c0cCyaKxdBQ0jD6eJVSjDD","authorship_tag":"ABX9TyMgcbBgr/fmMfScy0YecU45"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57RWkgR9ANZ7","executionInfo":{"status":"ok","timestamp":1612601918379,"user_tz":-330,"elapsed":5261,"user":{"displayName":"Neel Gajera","photoUrl":"","userId":"02429253171450337210"}},"outputId":"c4fa1e5f-56fd-498c-c593-fd0121032e45"},"source":["!git clone https://github.com/neelgajera/human-activity-recognizer.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'human-activity-recognizer'...\n","remote: Enumerating objects: 65, done.\u001b[K\n","remote: Counting objects: 100% (65/65), done.\u001b[K\n","remote: Compressing objects: 100% (60/60), done.\u001b[K\n","remote: Total 65 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (65/65), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jP_vCNkWAcAo"},"source":[" !cp \"/content/drive/MyDrive/Models\" -r \"/content/human-activity-recognizer\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SP1sGA5L6xz"},"source":["plaese download models file put  in project folder [models](https://drive.google.com/file/d/1nTRDi0hU5kLEldrJbZtteynzJWObKwOL/view?usp=sharing)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axKVGU72uL4P","executionInfo":{"status":"ok","timestamp":1612601935579,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Neel Gajera","photoUrl":"","userId":"02429253171450337210"}},"outputId":"7a675425-2b27-474c-f254-dc7056953bc3"},"source":["%cd /content/human-activity-recognizer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/human-activity-recognizer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wMcgYnB4KcDk"},"source":["!pip install py2sms\r\n","username =\"\"\r\n","password = \"\"\r\n","number = \"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XCbOAWVLhdM"},"source":["enter username and pasword for way2sms and mobile number "]},{"cell_type":"markdown","metadata":{"id":"sRrRp038K9bT"},"source":["this code for anonate video"]},{"cell_type":"code","metadata":{"id":"OTxQXru8GLBY"},"source":["from google.colab.patches import cv2_imshow\r\n","from IPython.display import clear_output\r\n","import os\r\n","import cv2\r\n","import py2sms\r\n","import time\r\n","import torch\r\n","import argparse\r\n","import numpy as np\r\n","from Detection.Utils import ResizePadding\r\n","from Track.Tracker import Detection, Tracker\r\n","from DetectorLoader import TinyYOLOv3_onecls\r\n","from PoseEstimateLoader import SPPE_FastPose\r\n","from fn import draw_single\r\n","from Track.Tracker import Detection, Tracker\r\n","from ActionsEstLoader import TSSTG\r\n","def preproc(image):\r\n","    image = resize_fn(image)\r\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n","    return image\r\n","def kpt2bbox(kpt, ex=20):\r\n","    return np.array((kpt[:, 0].min() - ex, kpt[:, 1].min() - ex,\r\n","                     kpt[:, 0].max() + ex, kpt[:, 1].max() + ex))\r\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n","detect_model = TinyYOLOv3_onecls(384,device=device)\r\n","max_age = 30\r\n","resize_fn = ResizePadding(384,384)\r\n","tracker = Tracker(max_age=max_age, n_init=3)\r\n","inp_pose = (224,116)\r\n","pose_model = SPPE_FastPose(\"resnet50\", inp_pose[0], inp_pose[1], device=device)\r\n","action_model = TSSTG()\r\n","fps_time = 0\r\n","cap = cv2.VideoCapture(\"videoplayback.mp4\")\r\n","f = 0\r\n","while(True):\r\n","    f += 1\r\n","    ret, frame = cap.read()\r\n","    if ret:\r\n","        frame = preproc(frame)\r\n","        detected = detect_model.detect(frame, need_resize=False, expand_bb=10)\r\n","        print(detected)\r\n","        tracker.predict()\r\n","        for track in tracker.tracks:\r\n","                det = torch.tensor([track.to_tlbr().tolist() + [0.5, 1.0, 0.0]], dtype=torch.float32)\r\n","                detected = torch.cat([detected, det], dim=0) if detected is not None else det\r\n","        detections = []\r\n","        if detected is not None:\r\n","            poses = pose_model.predict(frame, detected[:, 0:4], detected[:, 4])\r\n","            detections = [Detection(kpt2bbox(ps['keypoints'].numpy()),np.concatenate((ps['keypoints'].numpy(), ps['kp_score'].numpy()), axis=1),ps['kp_score'].mean().numpy()) for ps in poses]\r\n","            for bb in detected[:, 0:5]:\r\n","                frame = cv2.rectangle(frame, (bb[0], bb[1]), (bb[2], bb[3]), (0, 0, 255), 1)\r\n","\r\n","\r\n","        tracker.update(detections)\r\n","        for i, track in enumerate(tracker.tracks):\r\n","            if not track.is_confirmed():\r\n","                continue\r\n","\r\n","            track_id = track.track_id\r\n","            bbox = track.to_tlbr().astype(int)\r\n","            center = track.get_center().astype(int)\r\n","            action = 'pending..'\r\n","            clr = (0, 255, 0)\r\n","                # Use 30 frames time-steps to prediction.\r\n","            if len(track.keypoints_list) == 30:\r\n","                pts = np.array(track.keypoints_list, dtype=np.float32)\r\n","                out = action_model.predict(pts, frame.shape[:2])\r\n","                action_name = action_model.class_names[out[0].argmax()]\r\n","                action = '{}: {:.2f}%'.format(action_name, out[0].max() * 100)\r\n","                if action_name == 'Fall Down':\r\n","                    s=py2sms.send(username,password,number,\"fall down\")\r\n","                    clr = (255, 0, 0)\r\n","                elif action_name == 'Lying Down':\r\n","                    clr = (255, 200, 0)\r\n","\r\n","                # VISUALIZE.\r\n","            if track.time_since_update == 0:\r\n","                frame = draw_single(frame, track.keypoints_list[-1])\r\n","                frame = cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 1)\r\n","                frame = cv2.putText(frame, str(track_id), (center[0], center[1]), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 0, 0), 2)\r\n","                frame = cv2.putText(frame, action, (bbox[0] + 5, bbox[1] + 15), cv2.FONT_HERSHEY_COMPLEX, 0.4, clr, 1)\r\n","\r\n","        frame = cv2.resize(frame, (0, 0), fx=2., fy=2.)\r\n","        frame = cv2.putText(frame, '%d, FPS: %f' % (f, 1.0 / (time.time() - fps_time)),(10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\r\n","        frame = frame[:, :, ::-1]\r\n","        fps_time = time.time()\r\n","        cv2_imshow(frame)\r\n","        \r\n","        clear_output(wait=True)\r\n","        time.sleep(0.5)\r\n","\r\n","    if cv2.waitKey(30) & 0xFF == ord('q'):\r\n","            breakpoint\r\n","cap.stop()        \r\n","cv2.destroyAllWindows()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AFbKD0H7K7EB"},"source":["this is live cam action"]},{"cell_type":"code","metadata":{"id":"QDDXn4-4SP3M"},"source":["from IPython.display import display, Javascript, Image\r\n","from google.colab.output import eval_js\r\n","from base64 import b64decode, b64encode\r\n","import cv2\r\n","import numpy as np\r\n","import PIL\r\n","import io\r\n","import html\r\n","import time\r\n","import py2sms\r\n","import os\r\n","import torch\r\n","from Detection.Utils import ResizePadding\r\n","from Track.Tracker import Detection, Tracker\r\n","from DetectorLoader import TinyYOLOv3_onecls\r\n","from PoseEstimateLoader import SPPE_FastPose\r\n","from fn import draw_single\r\n","from Track.Tracker import Detection, Tracker\r\n","from ActionsEstLoader import TSSTG"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvhqykN0W5ss"},"source":["def js_to_image(js_reply):\r\n","  \"\"\"\r\n","  Params:\r\n","          js_reply: JavaScript object containing image from webcam\r\n","  Returns:\r\n","          img: OpenCV BGR image\r\n","  \"\"\"\r\n","  # decode base64 image\r\n","  image_bytes = b64decode(js_reply.split(',')[1])\r\n","  # convert bytes to numpy array\r\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\r\n","  # decode numpy array into OpenCV BGR image\r\n","  img = cv2.imdecode(jpg_as_np, flags=1)\r\n","  image = resize_fn(img)\r\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n","    \r\n","  return image\r\n","\r\n","# function to convert  image into base64 byte string to be overlayed on video stream\r\n","def bbox_to_bytes(bbox_array):\r\n","  \"\"\"\r\n","  Params:\r\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\r\n","  Returns:\r\n","        bytes: Base64 image byte string\r\n","  \"\"\"\r\n","  is_success, im_buf_arr = cv2.imencode(\".png\", bbox_array)\r\n","  byte_im = im_buf_arr.tobytes()\r\n","  \r\n","  # format return string\r\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(byte_im), 'utf-8')))\r\n","\r\n","  return bbox_bytes\r\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSy9gdoFYCbL"},"source":["def video_stream():\r\n","  js = Javascript('''\r\n","    var video;\r\n","    var div = null;\r\n","    var stream;\r\n","    var captureCanvas;\r\n","    var imgElement;\r\n","    var labelElement;\r\n","    \r\n","    var pendingResolve = null;\r\n","    var shutdown = false;\r\n","    \r\n","    function removeDom() {\r\n","       stream.getVideoTracks()[0].stop();\r\n","       video.remove();\r\n","       div.remove();\r\n","       video = null;\r\n","       div = null;\r\n","       stream = null;\r\n","       imgElement = null;\r\n","       captureCanvas = null;\r\n","       labelElement = null;\r\n","    }\r\n","    \r\n","    function onAnimationFrame() {\r\n","      if (!shutdown) {\r\n","        window.requestAnimationFrame(onAnimationFrame);\r\n","      }\r\n","      if (pendingResolve) {\r\n","        var result = \"\";\r\n","        if (!shutdown) {\r\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\r\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\r\n","        }\r\n","        var lp = pendingResolve;\r\n","        pendingResolve = null;\r\n","        lp(result);\r\n","      }\r\n","    }\r\n","    \r\n","    async function createDom() {\r\n","      if (div !== null) {\r\n","        return stream;\r\n","      }\r\n","\r\n","      div = document.createElement('div');\r\n","      div.style.border = '2px solid black';\r\n","      div.style.padding = '3px';\r\n","      div.style.width = '100%';\r\n","      div.style.maxWidth = '600px';\r\n","      document.body.appendChild(div);\r\n","      \r\n","      const modelOut = document.createElement('div');\r\n","      modelOut.innerHTML = \"<span>Status:</span>\";\r\n","      labelElement = document.createElement('span');\r\n","      labelElement.innerText = 'No data';\r\n","      labelElement.style.fontWeight = 'bold';\r\n","      modelOut.appendChild(labelElement);\r\n","      div.appendChild(modelOut);\r\n","           \r\n","      video = document.createElement('video');\r\n","      video.style.display = 'block';\r\n","      video.width = div.clientWidth - 6;\r\n","      video.setAttribute('playsinline', '');\r\n","      video.onclick = () => { shutdown = true; };\r\n","      stream = await navigator.mediaDevices.getUserMedia(\r\n","          {video: { facingMode: \"environment\"}});\r\n","      div.appendChild(video);\r\n","\r\n","      imgElement = document.createElement('img');\r\n","      imgElement.style.position = 'absolute';\r\n","      imgElement.style.zIndex = 1;\r\n","      imgElement.onclick = () => { shutdown = true; };\r\n","      div.appendChild(imgElement);\r\n","      \r\n","      const instruction = document.createElement('div');\r\n","      instruction.innerHTML = \r\n","          '<span style=\"color: red; font-weight: bold;\">' +\r\n","          'When finished, click here or on the video to stop this demo</span>';\r\n","      div.appendChild(instruction);\r\n","      instruction.onclick = () => { shutdown = true; };\r\n","      \r\n","      video.srcObject = stream;\r\n","      await video.play();\r\n","\r\n","      captureCanvas = document.createElement('canvas');\r\n","      captureCanvas.width = 640; //video.videoWidth;\r\n","      captureCanvas.height = 480; //video.videoHeight;\r\n","      window.requestAnimationFrame(onAnimationFrame);\r\n","      \r\n","      return stream;\r\n","    }\r\n","    async function stream_frame(label, imgData) {\r\n","      if (shutdown) {\r\n","        removeDom();\r\n","        shutdown = false;\r\n","        return '';\r\n","      }\r\n","\r\n","      var preCreate = Date.now();\r\n","      stream = await createDom();\r\n","      \r\n","      var preShow = Date.now();\r\n","      if (label != \"\") {\r\n","        labelElement.innerHTML = label;\r\n","      }\r\n","            \r\n","      if (imgData != \"\") {\r\n","        var videoRect = video.getClientRects()[0];\r\n","        imgElement.style.top = videoRect.top + \"px\";\r\n","        imgElement.style.left = videoRect.left + \"px\";\r\n","        imgElement.style.width = videoRect.width + \"px\";\r\n","        imgElement.style.height = videoRect.height + \"px\";\r\n","        imgElement.src = imgData;\r\n","      }\r\n","      \r\n","      var preCapture = Date.now();\r\n","      var result = await new Promise(function(resolve, reject) {\r\n","        pendingResolve = resolve;\r\n","      });\r\n","      shutdown = false;\r\n","      \r\n","      return {'create': preShow - preCreate, \r\n","              'show': preCapture - preShow, \r\n","              'capture': Date.now() - preCapture,\r\n","              'img': result};\r\n","    }\r\n","    ''')\r\n","\r\n","  display(js)\r\n","  \r\n","def video_frame(label, bbox):\r\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\r\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxTWY_AkYK7s"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n","detect_model = TinyYOLOv3_onecls(384,device=device)\r\n","max_age = 30\r\n","resize_fn = ResizePadding(384,384)\r\n","tracker = Tracker(max_age=max_age, n_init=3)\r\n","inp_pose = (224,116)\r\n","pose_model = SPPE_FastPose(\"resnet50\", inp_pose[0], inp_pose[1], device=device)\r\n","action_model = TSSTG()\r\n","fps_time = 0\r\n","f = 0\r\n","def kpt2bbox(kpt, ex=20):\r\n","    return np.array((kpt[:, 0].min() - ex, kpt[:, 1].min() - ex,\r\n","                     kpt[:, 0].max() + ex, kpt[:, 1].max() + ex))\r\n","video_stream()\r\n","# label for video\r\n","label_html = 'Capturing...'\r\n","# initialze bounding box to empty\r\n","bbox = ''\r\n","count = 0 \r\n","while True:\r\n","    f += 1\r\n","    js_reply = video_frame(label_html, bbox)\r\n","    if not js_reply:\r\n","        break\r\n","\r\n","    # convert JS response to OpenCV Image\r\n","    frame = js_to_image(js_reply[\"img\"])\r\n","    detected = detect_model.detect(frame, need_resize=False, expand_bb=10)\r\n","    tracker.predict()\r\n","    for track in tracker.tracks:\r\n","      det = torch.tensor([track.to_tlbr().tolist() + [0.5, 1.0, 0.0]], dtype=torch.float32)\r\n","      detected = torch.cat([detected, det], dim=0) if detected is not None else det\r\n","    detections = []\r\n","    if detected is not None:\r\n","      poses = pose_model.predict(frame, detected[:, 0:4], detected[:, 4])\r\n","      detections = [Detection(kpt2bbox(ps['keypoints'].numpy()),\r\n","                                    np.concatenate((ps['keypoints'].numpy(),\r\n","                                                    ps['kp_score'].numpy()), axis=1),\r\n","                                    ps['kp_score'].mean().numpy()) for ps in poses]\r\n","      for bb in detected[:, 0:5]:\r\n","        frame = cv2.rectangle(frame, (bb[0], bb[1]), (bb[2], bb[3]), (0, 0, 255), 1)\r\n","\r\n","\r\n","    tracker.update(detections)\r\n","    for i, track in enumerate(tracker.tracks):\r\n","          if not track.is_confirmed():\r\n","              continue\r\n","\r\n","          track_id = track.track_id\r\n","          bbox = track.to_tlbr().astype(int)\r\n","          center = track.get_center().astype(int)\r\n","          action = 'pending..'\r\n","          clr = (0, 255, 0)\r\n","                # Use 30 frames time-steps to prediction.\r\n","          if len(track.keypoints_list) == 30:\r\n","                pts = np.array(track.keypoints_list, dtype=np.float32)\r\n","                out = action_model.predict(pts, frame.shape[:2])\r\n","                action_name = action_model.class_names[out[0].argmax()]\r\n","                action = '{}: {:.2f}%'.format(action_name, out[0].max() * 100)\r\n","                if action_name == 'Fall Down':\r\n","                    s=py2sms.send(username,password,number,\"fall down\")\r\n","                    clr = (255, 0, 0)\r\n","                elif action_name == 'Lying Down':\r\n","                    clr = (255, 200, 0)\r\n","\r\n","                # VISUALIZE.\r\n","          if track.time_since_update == 0:\r\n","                frame = draw_single(frame, track.keypoints_list[-1])\r\n","                frame = cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 1)\r\n","                frame = cv2.putText(frame, str(track_id), (center[0], center[1]), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 0, 0), 2)\r\n","                frame = cv2.putText(frame, action, (bbox[0] + 5, bbox[1] + 15), cv2.FONT_HERSHEY_COMPLEX, 0.4, clr, 1)\r\n","\r\n","    frame = cv2.resize(frame, (0, 0), fx=2., fy=2.)\r\n","    frame = cv2.putText(frame, '%d, FPS: %f' % (f, 1.0 / (time.time() - fps_time)),(10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\r\n","    frame = frame[:, :, ::-1]\r\n","    fps_time = time.time()\r\n","    \r\n","\r\n","    \r\n","    # convert overlay of bbox into bytes\r\n","    bbox_bytes = bbox_to_bytes(frame)\r\n","    # update bbox so next frame gets new overlay\r\n","    bbox = bbox_bytes"],"execution_count":null,"outputs":[]}]}